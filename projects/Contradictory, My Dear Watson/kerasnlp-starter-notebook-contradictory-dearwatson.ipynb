{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://keras.io/img/logo-small.png\" alt=\"Keras logo\" width=\"100\"><br/>\n",
    "This starter notebook is provided by the Keras team.</center>\n",
    "\n",
    "## Keras NLP starter guide here: https://keras.io/guides/keras_nlp/getting_started/\n",
    "\n",
    "__This starter notebook uses the [BERT](https://arxiv.org/abs/1810.04805) pretrained model from KerasNLP.__\n",
    "\n",
    "**BERT** stands for **Bidirectional Encoder Representations from Transformers**. BERT and other Transformer encoder architectures have been wildly successful on a variety of tasks in NLP (natural language processing). They compute vector-space representations of natural language that are suitable for use in deep learning models.\n",
    "\n",
    "The BERT family of models uses the **Transformer encoder architecture** to process each token of input text in the full context of all tokens before and after, hence the name: Bidirectional Encoder Representations from Transformers.\n",
    "\n",
    "BERT models are usually pre-trained on a large corpus of text, then fine-tuned for specific tasks.\n",
    "\n",
    "\n",
    "![BERT Architecture](http://miro.medium.com/v2/resize:fit:1032/0*x3vhaoJdGndvZqmL.png)\n",
    "\n",
    "\n",
    "This notebook contains complete code to fine-tune BERT to perform a **Natural Language Inferencing (NLI)** model. NLI is a popular NLP problem that involves determining how pairs of sentences (consisting of a premise and a hypothesis) are related.\n",
    "\n",
    "Our **NLI model** will assign labels of 0, 1, or 2 (corresponding to **entailment, neutral, and contradiction**) to pairs of premises and hypotheses.\n",
    "\n",
    "Note that the train and test set include text in **fifteen different languages**!\n",
    "\n",
    "\n",
    "In this notebook, you will:\n",
    "\n",
    "- Load the Contradictory, My Dear Watson dataset\n",
    "- Explore the dataset\n",
    "- Preprocess the data\n",
    "- Load a BERT model from Keras NLP\n",
    "- Train your own model, fine-tuning BERT as part of that\n",
    "- Generate the submission file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-11-16T23:15:45.607610Z",
     "iopub.status.busy": "2025-11-16T23:15:45.607385Z",
     "iopub.status.idle": "2025-11-16T23:17:57.386121Z",
     "shell.execute_reply": "2025-11-16T23:17:57.385007Z",
     "shell.execute_reply.started": "2025-11-16T23:15:45.607588Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install -q keras-nlp --upgrade\n",
    "!pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-16T23:18:08.396809Z",
     "iopub.status.busy": "2025-11-16T23:18:08.396452Z",
     "iopub.status.idle": "2025-11-16T23:18:09.431427Z",
     "shell.execute_reply": "2025-11-16T23:18:09.430467Z",
     "shell.execute_reply.started": "2025-11-16T23:18:08.396780Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected exception formatting exception. Falling back to standard exception\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3508, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/ipykernel_34/1197178146.py\", line 3, in <module>\n",
      "    import tensorflow as tf\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/tensorflow/__init__.py\", line 49, in <module>\n",
      "    from tensorflow._api.v2 import __internal__\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/tensorflow/_api/v2/__internal__/__init__.py\", line 8, in <module>\n",
      "    from tensorflow._api.v2.__internal__ import autograph\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/tensorflow/_api/v2/__internal__/autograph/__init__.py\", line 8, in <module>\n",
      "    from tensorflow.python.autograph.core.ag_ctx import control_status_ctx # line: 34\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/tensorflow/python/autograph/core/ag_ctx.py\", line 21, in <module>\n",
      "    from tensorflow.python.autograph.utils import ag_logging\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/tensorflow/python/autograph/utils/__init__.py\", line 17, in <module>\n",
      "    from tensorflow.python.autograph.utils.context_managers import control_dependency_on_returns\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/tensorflow/python/autograph/utils/context_managers.py\", line 19, in <module>\n",
      "    from tensorflow.python.framework import ops\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/tensorflow/python/framework/ops.py\", line 50, in <module>\n",
      "    from tensorflow.python.eager import context\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/tensorflow/python/eager/context.py\", line 38, in <module>\n",
      "    from tensorflow.python.eager import execute\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/tensorflow/python/eager/execute.py\", line 21, in <module>\n",
      "    from tensorflow.python.framework import dtypes\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/tensorflow/python/framework/dtypes.py\", line 21, in <module>\n",
      "    import ml_dtypes\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/ml_dtypes/__init__.py\", line 40, in <module>\n",
      "    from ml_dtypes._finfo import finfo\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/ml_dtypes/_finfo.py\", line 153, in <module>\n",
      "    class finfo(np.finfo):  # pylint: disable=invalid-name,missing-class-docstring\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/ml_dtypes/_finfo.py\", line 699, in finfo\n",
      "    _finfo_cache = {\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/ml_dtypes/_finfo.py\", line 700, in <dictcomp>\n",
      "    t: init_fn.__func__() for t, init_fn in _finfo_type_map.items()  # pytype: disable=attribute-error\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/ml_dtypes/_finfo.py\", line 668, in _float8_e8m0fnu_finfo\n",
      "    if not hasattr(obj, \"tiny\"):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/numpy/core/getlimits.py\", line 578, in tiny\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/numpy/core/getlimits.py\", line 557, in smallest_normal\n",
      "TypeError: ufunc 'isnan' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe''\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 2105, in showtraceback\n",
      "    stb = self.InteractiveTB.structured_traceback(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/ultratb.py\", line 1428, in structured_traceback\n",
      "    return FormattedTB.structured_traceback(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/ultratb.py\", line 1319, in structured_traceback\n",
      "    return VerboseTB.structured_traceback(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/ultratb.py\", line 1172, in structured_traceback\n",
      "    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/ultratb.py\", line 1087, in format_exception_as_a_whole\n",
      "    frames.append(self.format_record(record))\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/ultratb.py\", line 969, in format_record\n",
      "    frame_info.lines, Colors, self.has_colors, lvals\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/ultratb.py\", line 792, in lines\n",
      "    return self._sd.lines\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/stack_data/utils.py\", line 144, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/stack_data/core.py\", line 734, in lines\n",
      "    pieces = self.included_pieces\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/stack_data/utils.py\", line 144, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/stack_data/core.py\", line 681, in included_pieces\n",
      "    pos = scope_pieces.index(self.executing_piece)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/stack_data/utils.py\", line 144, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/stack_data/core.py\", line 660, in executing_piece\n",
      "    return only(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/executing/executing.py\", line 190, in only\n",
      "    raise NotOneValueFound('Expected one value, found 0')\n",
      "executing.executing.NotOneValueFound: Expected one value, found 0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import keras_nlp\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "print(\"KerasNLP version:\", keras_nlp.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accelerator\n",
    "\n",
    "Detect hardware, return appropriate distribution strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-16T23:14:49.428485Z",
     "iopub.status.busy": "2025-11-16T23:14:49.428058Z",
     "iopub.status.idle": "2025-11-16T23:14:50.153648Z",
     "shell.execute_reply": "2025-11-16T23:14:50.152737Z",
     "shell.execute_reply.started": "2025-11-16T23:14:49.428455Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Deallocate tpu buffers before initializing tpu system.\n",
      "INFO:tensorflow:Initializing the TPU system: local\n"
     ]
    },
    {
     "ename": "NotFoundError",
     "evalue": "TPUs not found in the cluster. Failed in initialization: No matching devices found for '/device:TPU_SYSTEM:0' [Op:__inference__tpu_init_fn_5]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "File \u001b[0;32m/usr/local/lib/python3.8/site-packages/tensorflow/python/tpu/tpu_strategy_util.py:127\u001b[0m, in \u001b[0;36minitialize_tpu_system\u001b[0;34m(cluster_resolver)\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ops\u001b[38;5;241m.\u001b[39mdevice(tpu\u001b[38;5;241m.\u001b[39m_tpu_system_device_name(job)):  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m--> 127\u001b[0m   output \u001b[38;5;241m=\u001b[39m \u001b[43m_tpu_init_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    128\u001b[0m context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/site-packages/tensorflow/python/util/traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/site-packages/tensorflow/python/eager/execute.py:52\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 52\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[1;32m     53\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: No matching devices found for '/device:TPU_SYSTEM:0' [Op:__inference__tpu_init_fn_5]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m      2\u001b[0m    \u001b[38;5;66;03m# detect and init the TPU\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m    tpu \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdistribute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcluster_resolver\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTPUClusterResolver\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m    \u001b[38;5;66;03m# instantiate a distribution strategy\u001b[39;00m\n\u001b[1;32m      5\u001b[0m    strategy \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mdistribute\u001b[38;5;241m.\u001b[39mTPUStrategy(tpu)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/site-packages/tensorflow/python/distribute/cluster_resolver/tpu/tpu_cluster_resolver.py:110\u001b[0m, in \u001b[0;36mTPUClusterResolver.connect\u001b[0;34m(tpu, zone, project)\u001b[0m\n\u001b[1;32m    108\u001b[0m remote\u001b[38;5;241m.\u001b[39mconnect_to_cluster(resolver)\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtpu\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tpu_strategy_util  \u001b[38;5;66;03m# pylint: disable=g-import-not-at-top\u001b[39;00m\n\u001b[0;32m--> 110\u001b[0m \u001b[43mtpu_strategy_util\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minitialize_tpu_system\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresolver\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resolver\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/site-packages/tensorflow/python/tpu/tpu_strategy_util.py:130\u001b[0m, in \u001b[0;36minitialize_tpu_system\u001b[0;34m(cluster_resolver)\u001b[0m\n\u001b[1;32m    128\u001b[0m   context\u001b[38;5;241m.\u001b[39masync_wait()\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m errors\u001b[38;5;241m.\u001b[39mInvalidArgumentError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 130\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m errors\u001b[38;5;241m.\u001b[39mNotFoundError(\n\u001b[1;32m    131\u001b[0m       \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    132\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTPUs not found in the cluster. Failed in initialization: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    133\u001b[0m       \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(e))\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    135\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m run_eagerly \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mNotFoundError\u001b[0m: TPUs not found in the cluster. Failed in initialization: No matching devices found for '/device:TPU_SYSTEM:0' [Op:__inference__tpu_init_fn_5]"
     ]
    }
   ],
   "source": [
    " try:\n",
    "    # detect and init the TPU\n",
    "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver.connect()\n",
    "    # instantiate a distribution strategy\n",
    "    strategy = tf.distribute.TPUStrategy(tpu)\n",
    "except ValueError:\n",
    "    print(\"TPU not activated\")\n",
    "    strategy = tf.distribute.MirroredStrategy() # Works on CPU, single GPU and multiple GPUs in a single VM.\n",
    "\n",
    "print(\"replicas:\", strategy.num_replicas_in_sync)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the Contradictory, My Dear Watson dataset\n",
    "Let's have a look at all the data files\n",
    "\n",
    "The training set contains a premise, a hypothesis, a label (0 = entailment, 1 = neutral, 2 = contradiction), and the language of the text. For more information about what these mean and how the data is structured, check out the data page: https://www.kaggle.com/c/contradictory-my-dear-watson/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-03T12:41:52.594994Z",
     "iopub.status.busy": "2023-08-03T12:41:52.594663Z",
     "iopub.status.idle": "2023-08-03T12:41:52.603048Z",
     "shell.execute_reply": "2023-08-03T12:41:52.602032Z",
     "shell.execute_reply.started": "2023-08-03T12:41:52.594965Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "DATA_DIR = '/kaggle/input/contradictory-my-dear-watson/'\n",
    "\n",
    "RESULT_DICT = {\n",
    "    0 : \"entailment\",\n",
    "    1 : \"neutral\",\n",
    "    2 : \"contradiction\"\n",
    "}\n",
    "\n",
    "for dirname, _, filenames in os.walk(DATA_DIR):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-03T12:41:52.6045Z",
     "iopub.status.busy": "2023-08-03T12:41:52.604173Z",
     "iopub.status.idle": "2023-08-03T12:41:52.735395Z",
     "shell.execute_reply": "2023-08-03T12:41:52.734261Z",
     "shell.execute_reply.started": "2023-08-03T12:41:52.604471Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(DATA_DIR + \"train.csv\")\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-03T12:41:52.738507Z",
     "iopub.status.busy": "2023-08-03T12:41:52.738018Z",
     "iopub.status.idle": "2023-08-03T12:41:52.794524Z",
     "shell.execute_reply": "2023-08-03T12:41:52.793508Z",
     "shell.execute_reply.started": "2023-08-03T12:41:52.738472Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df_test = pd.read_csv(DATA_DIR + \"test.csv\")\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at some pairs of sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-03T12:41:52.796189Z",
     "iopub.status.busy": "2023-08-03T12:41:52.795829Z",
     "iopub.status.idle": "2023-08-03T12:41:52.808622Z",
     "shell.execute_reply": "2023-08-03T12:41:52.807665Z",
     "shell.execute_reply.started": "2023-08-03T12:41:52.796158Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def display_pair_of_sentence(x):\n",
    "    print( \"Premise : \" + x['premise'])\n",
    "    print( \"Hypothesis: \" + x['hypothesis'])\n",
    "    print( \"Language: \" + x['language'])\n",
    "    print( \"Label: \" + str(x['label']))\n",
    "    print()\n",
    "\n",
    "df_train.head(10).apply(lambda x : display_pair_of_sentence(x), axis=1)\n",
    "\n",
    "df_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore the dataset\n",
    "\n",
    "Let's look at the distribution of labels in the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-03T12:41:52.810327Z",
     "iopub.status.busy": "2023-08-03T12:41:52.809787Z",
     "iopub.status.idle": "2023-08-03T12:41:53.132724Z",
     "shell.execute_reply": "2023-08-03T12:41:53.131619Z",
     "shell.execute_reply.started": "2023-08-03T12:41:52.810268Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Initialize the matplotlib figure\n",
    "f, ax = plt.subplots(figsize=(12, 4))\n",
    "\n",
    "# Plot the total crashes\n",
    "sns.set_color_codes(\"pastel\")\n",
    "sns.despine()\n",
    "ax = sns.countplot(data=df_train,\n",
    "                   y=\"label\",\n",
    "                   order = df_train['label'].value_counts().index)\n",
    "\n",
    "abs_values = df_train['label'].value_counts(ascending=False)\n",
    "rel_values = df_train['label'].value_counts(ascending=False, normalize=True).values * 100\n",
    "lbls = [f'{p[0]} ({p[1]:.0f}%)' for p in zip(abs_values, rel_values)]\n",
    "\n",
    "ax.bar_label(container=ax.containers[0], labels=lbls)\n",
    "\n",
    "ax.set_yticklabels([RESULT_DICT[index] for index in abs_values.index])\n",
    "\n",
    "ax.set_title(\"Distribution of labels in the training set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the distribution of languages in the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-03T12:41:53.134389Z",
     "iopub.status.busy": "2023-08-03T12:41:53.134015Z",
     "iopub.status.idle": "2023-08-03T12:41:53.605813Z",
     "shell.execute_reply": "2023-08-03T12:41:53.60486Z",
     "shell.execute_reply.started": "2023-08-03T12:41:53.134359Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Initialize the matplotlib figure\n",
    "f, ax = plt.subplots(figsize=(10, 10))\n",
    "\n",
    "# Plot the total crashes\n",
    "sns.set_color_codes(\"pastel\")\n",
    "sns.despine()\n",
    "ax = sns.countplot(data=df_train,\n",
    "                   y=\"language\",\n",
    "                   order = df_train['language'].value_counts().index)\n",
    "\n",
    "abs_values = df_train['language'].value_counts(ascending=False)\n",
    "rel_values = df_train['language'].value_counts(ascending=False, normalize=True).values * 100\n",
    "lbls = [f'{p[0]} ({p[1]:.0f}%)' for p in zip(abs_values, rel_values)]\n",
    "\n",
    "ax.bar_label(container=ax.containers[0], labels=lbls)\n",
    "\n",
    "ax.set_title(\"Distribution of languages in the training set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the length of the sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-03T12:41:53.607265Z",
     "iopub.status.busy": "2023-08-03T12:41:53.606974Z",
     "iopub.status.idle": "2023-08-03T12:41:53.64704Z",
     "shell.execute_reply": "2023-08-03T12:41:53.646139Z",
     "shell.execute_reply.started": "2023-08-03T12:41:53.607239Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df_train[\"premise_length\"] = df_train[\"premise\"].apply(lambda x : len(x))\n",
    "df_train[\"hypothesis_length\"] = df_train[\"hypothesis\"].apply(lambda x : len(x))\n",
    "df_train[[\"hypothesis_length\", \"premise_length\"]].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess the data\n",
    "\n",
    "Text inputs need to be transformed to numeric token ids and arranged in several Tensors before being input to BERT.\n",
    "\n",
    "The BertClassifier model can  be configured with a preprocessor layer, in which case it will automatically apply preprocessing to raw inputs during fit(), predict(), and evaluate(). This is done by default when creating the model with from_preset().\n",
    "\n",
    "Bert is only trained in English corpus. That's why people use multilingual Bert or XLM-Roberta for this competition.\n",
    "\n",
    "Here are some models for multi-language NLP available in Keras NLP:\n",
    "- bert_base_multi\n",
    "- deberta_v3_base_multi\n",
    "- distil_bert_base_multi\n",
    "- xlm_roberta_base_multi\n",
    "- xlm_roberta_large_multi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-03T12:41:53.648532Z",
     "iopub.status.busy": "2023-08-03T12:41:53.648078Z",
     "iopub.status.idle": "2023-08-03T12:41:53.653275Z",
     "shell.execute_reply": "2023-08-03T12:41:53.652347Z",
     "shell.execute_reply.started": "2023-08-03T12:41:53.648499Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "VALIDATION_SPLIT = 0.3\n",
    "TRAIN_SIZE = int(df_train.shape[0]*(1-VALIDATION_SPLIT))\n",
    "BATCH_SIZE = 16 * strategy.num_replicas_in_sync\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a utility function that splits the example into an `(x, y)` tuple that is suitable for `model.fit()`.\n",
    "\n",
    "By default, `keras_nlp.models.BertClassifier` will tokenize and pack together raw strings using a `\"[SEP]\"` token during training.\n",
    "\n",
    "Therefore, this label splitting is all the data preparation that we need to perform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-03T12:41:53.656142Z",
     "iopub.status.busy": "2023-08-03T12:41:53.655745Z",
     "iopub.status.idle": "2023-08-03T12:41:53.763476Z",
     "shell.execute_reply": "2023-08-03T12:41:53.762577Z",
     "shell.execute_reply.started": "2023-08-03T12:41:53.656113Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def split_labels(x, y):\n",
    "    return (x[0], x[1]), y\n",
    "\n",
    "\n",
    "training_dataset = (\n",
    "    tf.data.Dataset.from_tensor_slices(\n",
    "        (\n",
    "            df_train[['premise','hypothesis']].values,\n",
    "            keras.utils.to_categorical(df_train['label'], num_classes=3)\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "train_dataset = training_dataset.take(TRAIN_SIZE)\n",
    "val_dataset = training_dataset.skip(TRAIN_SIZE)\n",
    "\n",
    "# Apply the preprocessor to every sample of train, val and test data using `map()`.\n",
    "# [`tf.data.AUTOTUNE`](https://www.tensorflow.org/api_docs/python/tf/data/AUTOTUNE) and `prefetch()` are options to tune performance, see\n",
    "# https://www.tensorflow.org/guide/data_performance for details.\n",
    "\n",
    "train_preprocessed = train_dataset.map(split_labels, tf.data.AUTOTUNE).batch(BATCH_SIZE, drop_remainder=True).cache().prefetch(tf.data.AUTOTUNE)\n",
    "val_preprocessed = val_dataset.map(split_labels, tf.data.AUTOTUNE).batch(BATCH_SIZE, drop_remainder=True).cache().prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-31T05:23:02.390962Z",
     "iopub.status.busy": "2023-07-31T05:23:02.390523Z",
     "iopub.status.idle": "2023-07-31T05:23:02.396392Z",
     "shell.execute_reply": "2023-07-31T05:23:02.394937Z",
     "shell.execute_reply.started": "2023-07-31T05:23:02.390928Z"
    }
   },
   "source": [
    "# Load a BERT model from Keras NLP - Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-03T12:41:56.437808Z",
     "iopub.status.busy": "2023-08-03T12:41:56.437108Z",
     "iopub.status.idle": "2023-08-03T12:42:32.217655Z",
     "shell.execute_reply": "2023-08-03T12:42:32.216286Z",
     "shell.execute_reply.started": "2023-08-03T12:41:56.437768Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Load a BERT model.\n",
    "\n",
    "with strategy.scope():\n",
    "    classifier = keras_nlp.models.BertClassifier.from_preset(\"bert_base_multi\", num_classes=3)\n",
    "\n",
    "    # in distributed training, the recommendation is to scale batch size and learning rate with the numer of workers.\n",
    "    classifier.compile(optimizer=keras.optimizers.Adam(1e-5*strategy.num_replicas_in_sync),\n",
    "                       loss=keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "                       metrics=['accuracy'])\n",
    "\n",
    "    classifier.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train your own model - Fine-tuning BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-03T12:42:35.544099Z",
     "iopub.status.busy": "2023-08-03T12:42:35.543015Z",
     "iopub.status.idle": "2023-08-03T12:46:31.013639Z",
     "shell.execute_reply": "2023-08-03T12:46:31.012055Z",
     "shell.execute_reply.started": "2023-08-03T12:42:35.544055Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "EPOCHS=3\n",
    "history = classifier.fit(train_preprocessed,\n",
    "                         epochs=EPOCHS,\n",
    "                         validation_data=val_preprocessed\n",
    "                        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate the submission file\n",
    "\n",
    "Let's get the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-03T12:47:09.621045Z",
     "iopub.status.busy": "2023-08-03T12:47:09.619964Z",
     "iopub.status.idle": "2023-08-03T12:47:45.845082Z",
     "shell.execute_reply": "2023-08-03T12:47:45.843478Z",
     "shell.execute_reply.started": "2023-08-03T12:47:09.620998Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "predictions = classifier.predict((df_test['premise'],df_test['hypothesis']), batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we create a csv file with exactly 5195 entries plus a header row.\n",
    "\n",
    "The file has 2 columns:\n",
    "- id (sorted in any order)\n",
    "- prediction (contains your predictions: 0 for entailment, 1 for neutral, 2 for contradiction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-03T12:47:45.848742Z",
     "iopub.status.busy": "2023-08-03T12:47:45.848363Z",
     "iopub.status.idle": "2023-08-03T12:47:45.865675Z",
     "shell.execute_reply": "2023-08-03T12:47:45.864612Z",
     "shell.execute_reply.started": "2023-08-03T12:47:45.848708Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "submission = df_test.id.copy().to_frame()\n",
    "submission[\"prediction\"] = np.argmax(predictions, axis=1)\n",
    "\n",
    "submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "submission.to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you need details on how to submit from a notebook, review the FAQ on [\"How do I make a submission?\"]\n",
    "(https://www.kaggle.com/c/contradictory-my-dear-watson/overview/frequently-asked-questions)\n",
    "\n",
    "Good luck!\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 1408234,
     "sourceId": 21733,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30529,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
